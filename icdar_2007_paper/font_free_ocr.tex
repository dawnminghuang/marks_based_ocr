%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% FILE: font_free_ocr.tex
%
% DESCRIPTION: LaTeX source file of ICDAR 2007 paper submission
%
% CVS:
% $Id: font_free_ocr.tex,v 1.11 2007-02-03 20:52:18 roweis Exp $
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% INITIALIZATION %
%%%%%%%%%%%%%%%%%%
\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{latex8}
\usepackage{times}
\usepackage{graphicx}
\pagestyle{empty} %remove page numbers

% DOCUMENT START %
%%%%%%%%%%%%%%%%%%
\begin{document}

% TITLE %
%%%%%%%%%
\title{Shape-Free Statistical Information in Optical Character Recognition}

% AUTHOUR INFO %
%%%%%%%%%%%%%%%%
\author{First Author\\
Addr 1\\
Addr 2\\
Addr 3\\
\and
Second Author\\
}

%@@the following is not to be included during the review process
%\author{Scott Leishman\\
%Department of Computer Science\\
%University of Toronto\\ 
%scottl@cs.toronto.edu\\
%\and
%Sam Roweis\\
%Department of Computer Science\\
%University of Toronto\\
%roweis@cs.toronto.edu\\
%}

\maketitle
\thispagestyle{empty}


% ABSTRACT %
%%%%%%%%%%%%
\begin{abstract}
@@to be written@@
\end{abstract}


% INTRODUCTION %
%%%%%%%%%%%%%%%%
\Section{Introduction}

Traditional approaches to optical character recognition (OCR) have typically
relied on font-specific shape information to convert bitmap
images of glyphs into corresponding sequences of (ASCII or Unicode encoded)
characters.  Some of the earliest systems were heavily restricted, only
able to recognize images that happened to closely match a template culled from
a database containing a handful of common typefaces in typical point
sizes.  Modern systems have improved, and are now able to recognize
symbols from a much larger collection of varying fonts and character shapes.  
This has often been accomplished by using images or features of images from a 
wide array of font faces, styles, and sizes as labelled training data for 
training character classifiers.  Nonetheless, performance still degrades
in documents containing unusual typefaces or fonts sizes.  
As electronic publishing becomes increasingly common (including the
ability for ordinary users to create their own custom fonts), the enormous
variety of fonts seen in printed and online documents continues to grow rapidly.
This creates an ongoing motivation to pursue sources of {\em font and shape
independent} information useful for optical character recognition.

Many previous attempts at ``font free'' character recognition share
the same general structure. Each page of ink is segmented into components
roughly corresponding to individual graphemes of the 
underlying document language. These components are clustered together in an 
unsupervised fashion based only on visual similarity.  (For a language like 
English, the ideal result of this step would be a set of clusters such that
each one contains only occurrences of a particular 
character, digit, or punctuation symbol found in the document.
Of course, such exact clusterings are rarely achievable in practice, and 
trade-offs must be made between the final number of clusters and the 
consistency of the component members within a cluster.)
In one of the earliest shape-free recognition approaches, Nagy et al. treated
the clustered sequence of components as the input cipher-text of a 
cryptogram\cite{nagy1987}.  Clusters were then assigned plain-text labels, 
with the assignment guided by the frequency of matches to words looked up in a 
small dictionary.

%@@@ Talk about Lee and Hull HMM approach too?  perhaps

Building on this, Ho and Nagy attempted to infer cluster labels using a series 
of simple statistical modules\cite{ho2000}.  Each module scores potential 
assignments using what they call a ``v/p ratio'', which counts the number of 
valid dictionary words that match the partially assigned word patterns that 
contain the cluster to be mapped.  In subsequent work, they used 
several n-gram, frequency, and word-positional features to construct a 
classifier able to reliably determine whether an appropriately segmented 
cluster was an upper or lower case character, a digit, or a 
punctuation symbol\cite{ho2001}.

Recently, Huang et al. have used an entropy based approach to decode the
sequence of cluster identifiers representing individual words\cite{huang2006}.  
Confident mappings (those for which the distribution over character labels is 
sharply peaked at just a single label, given a dictionary of lookup words the 
same length as those that the cluster to be mapped appear in) are assigned 
first and used to narrow down the mappings for subsequent clusters.

% we should cite mccallum and miller from umass here
% do you mean something beyond their tech report (which is already cited above
% as huang2006)?

In this paper we explore the extent to which contextual and statistical sequence
information can be exploited in character recognition without making any
a priori use of character shape or font knowledge. We extend earlier
investigations by reporting results on all character types, not just
lowercase alphabetic characters. As well, we provide an analysis of
cases in which purely contextual information is not sufficient to
establish character identity.


%unfortunately this figure has to be moved to get it to appear on the same page
%as its description.
\begin{figure*}[ht]
  \centering
  \includegraphics[scale=1]{figures/input_lines}
  \includegraphics[scale=1]{figures/line_comps}
  \caption{Sample input image region and corresponding components, baselines,
           and x-heights found}
  \label{inimg_fig}
\end{figure*}

% CLUSTERING APPROACH %
%%%%%%%%%%%%%%%%%%%%%%%
\Section{Clustering Procedure}
\label{clustering_sec}

Before attempts can be made to recognize the sequence of glyphs that 
constitute input document images, the appropriate elementary symbol regions 
must first be located, isolated, and clustered together.\footnote{The
discussion that follows assumes the document images given are not
already ``symbolically compressed''. Such a compression scheme will
store a single template image and the offset locations of each
appearance of that particular shape, even grouping together
those shapes that are slightly varied.  For input document images that
have been compressed in this way (via a JBIG2 compliant encoder 
or parsing a PDF file for instance), a clustering procedure is often not necessary.}
After typical page image preprocessing steps like layout analysis, textual
region identification, deskewing, and noise removal are performed, the
connected components of ink on each page are identified and their bounding box
positions on the page are stored.  For each component, the nearest neighbouring
component in each of the four principal directions (top, bottom, left, and
right), as well as its pixel offset distance is also stored.  Individual lines
of text are then identified by following the chain of neighbouring components in
reading order.  The bounding box for a line is expanded as neighbour
component positions are read, and checks are made of neighbours in non-reading
order directions to determine if a line should be extended perpendicularly.
%
Since some symbols like {\em i, j, \'{e}, :, !} are made up of more than one
connected component, attempts are made to merge their constituent parts.  This
is accomplished by looking for separate components that belong to the same
line, are separated by a relatively small vertical distance (set manually), and
have one of the components completely overlapping the other horizontally.

Figure \ref{inimg_fig} illustrates this line and component finding process on a
small region of input text (top).  The bottom half of the figure shows that on
this particular region, almost every connected component corresponds to a
single character glyph.  The baseline and x-height of each line found is also
shown.

Each connected component image is initially assigned to its own cluster, then
clusters are merged in an agglomerative fashion.  First a simple Euclidean
distance measurement is taken between cluster centroid images, and those whose
value falls below a conservatively set threshold are merged together.  For near
noiseless documents with consistent symbol shapes, this has the effect of 
greatly reducing the number of clusters in a fairly efficient manner.
%
%As Figure \ref{eucdist_fig} shows, one of the problems in calculating the 
%Euclidean distance between two images is that each pixel difference is weighted 
%equally regardless of where it occurs (the bottom middle and right panes show
%the residual mismatched pixels for the corresponding binary input images above
%them).  As a result two images of the same symbol that are either noisy or 
%written in a slightly different font style or size could end up having a larger 
%distance than that between images of different symbols.
%
%\begin{figure}[ht]
%  \centering
%  \includegraphics[scale=0.3]{figures/euc_dist_comparisons}
%  \caption{Euclidean distance as calculated between a regular and italicized
%  {\em e} versus the regular {\em e} and an {\em o} in the same font}
%  \label{eucdist_fig}
%\end{figure}
%
%To attempt to combat the problems inherent in using Euclidean or other
%unweighted distance metrics, 
The clusters are subsequently refined using the
Hausdorff distance\cite{rucklidge1996}.  
%The Hausdorff distance between two 
%binary images $A$ and $B$ is typically defined as
%\begin{eqnarray}
%H(A,B) & = & \max(h(A,B), h(B,A))
%\end{eqnarray}
%where
%\begin{eqnarray}
%h(A,B) & = & \max_{a \in A} \min_{b \in B} || a - b ||
%\end{eqnarray}
%and $|| \cdot ||$ will be the $L_2$ (Euclidean) norm.
%
%$h(A,B)$ can be interpreted as the directed Hausdorff distance from image $A$ to
%image $B$, and means that for each foreground pixel in $A$ there is a foreground
%pixel in $B$ that is no farther than $h(A,B)$ away, when the two images are 
%laid over top one another.  
Because the cluster average images may not be 
binary, they are first thresholded so that they can be compared using
this metric.
%
%To speed up processing, the Hausdorff distance is only calculated at the most
%correlated position between each cluster average image.  This is found by
%convolving the comparison image with a long image created by combining the
%other cluster averages.  A Euclidean distance transform is then calculated on
%this long image, with copies of the comparison image laid on top at the
%appropriate positions, and the values at the 'on' pixels of the comparison 
%image read off.  The maximum such value read on the pixels of each
%comparison image defines the directed Hausdorff distance from the comparison 
%image to that particular image.  The Euclidean distance transform is then
%calculated on the comparison image, and a large tiled version is constructed
%with appropriate padding to ensure that the previous long image can be laid on
%top at the maximally correlated positions of each copy of the comparison image.
%The underlying values at the 'on' pixels of each of the other images is then
%read and the maximum taken to define the directed distance from the other
%images to the comparison image.  Finally, the overall Hausdorff distance is
%calculated by taking the maximum of these two directed distances for each
%image. 
Those that fall within a particular threshold ($\sqrt 2$ in our
experiments) are then clustered together.  Figure \ref{hausdist_fig}
shows this calculation between a lower case `e' and a lower case 'o', and
between the `e' and an italicized version of the same character.  The Euclidean
distance transform is calculated with the resultant values listed for each
pixel, and an image of the character to be compared is laid overtop.  The on 
pixel of the comparison character which lies farthest from any on pixel in the
original character is highlighted. 

\begin{figure}[ht]
  \centering
  \includegraphics[scale=.5]{figures/haus_dist_comparisons}
  \caption{Hausdorff distance as calculated between a regular and italicized
  {\em e} versus the regular {\em e} and an {\em o} in the same font}
  \label{hausdist_fig}
\end{figure}

It should be noted that no attempts are made to normalize characters since the
Hausdorff distance combined with the averaging that takes place as components
are added to the clusters, can typically handle small deviations in font size.
While this may result in multiple clusters for the same symbol if font sizes
differ drastically, contextual procedures outlined in the following section
should still result in correct symbol recognition for all such clusters.

Depending on the font face, amount of kerning employed, and the quality of the
input document image, some glyphs may be broken into two or more pieces, or
they may end up touching one another resulting in a single component containing
multiple symbols.
%
To handle glyphs that have been broken apart, we attempt to recombine them by
examining the clusters that they belong to.  If components belonging to a
particular cluster tend to always lie adjacent to components in a second
cluster (and vice versa), and if the typical distance between these components
is relatively small, then they are merged.  In our experiments, those clusters
for which at least 85\% of the components share a neighbouring component in the
same second cluster, and are no farther than 3 pixels apart are merged.

Attempts are made to separate glyphs that have been smeared or so tightly
kerned that they are touching, by iteratively splitting the glyph near a
suspected boundary and trying to match each half with other cluster averages.
Provided both halves lie within a small Euclidean distance of their matching
clusters, the components of the original cluster are split at the appropriate
position, and added to their respective matching clusters.

This Hausdorff matching, cluster merging and splitting procedure is repeated
over those clusters that have had at least one change to their number of 
components at the previous iteration, until no further changes are seen.

With symbol clustering complete, a cluster is created to represent the blank
space between each word of documents written in alphabetic languages.
Inter-word space width is estimated in a simple fashion by counting the
frequency of component neighbour distances.  This distribution is typically
bi-modal with the first mode representing inter-character spacing, and a second
(often smaller) mode representing inter-word spacing.  The width assigned
is based on this second modal point, but is underestimated slightly to ensure no
actual inter-word spaces are missed.  New components are created representing
each of these spaces, and neighbours are updated appropriately.

Figure \ref{clavg_fig} shows the resulting cluster averages after this
procedure has been run on a single document taken from the Department of Energy
Sample 3 OCR dataset created by UNLV's ISRI group\cite{nartker2005}.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=.5]{figures/cluster_averages}
  \caption{Typical cluster averages for a single document}
  \label{clavg_fig}
\end{figure}


% CONTEXTUAL RECOGNITION APPROACH %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Section{Contextual Recognition Based on Position}

With a reasonable clustering complete, the labels belonging to each cluster are
ready to be recognized.  Our contextual recognition approach has only been
tested on documents written in English, but should also be amenable to other
writing systems whose atomic symbols are roughly phonetic.  In our experiments
we have also assumed that we know in advance the language of the
input document, however previous research has shown it possible to infer this
automatically with relatively good accuracy\cite{sibun1994},\cite{hochberg1997}.

First a large text corpus is used to estimate overall word and symbol frequency,
as well as symbol positional frequency.  We take counts of the number of times
each symbol appears in each position of words up to a particular length.  As
a result of this, each symbol will then define a point in an $\frac{x
(x+1)}{2}$ dimensional ``positional'' feature space, where $x$ defines the
maximum word length to include (15 in our experiments).  We normalize the
positional counts for each symbol so that within each word length, we end up
with a distribution over positional frequency.  Figure \ref{e_pos_fig} shows
this resultant feature vector for the symbol `e' when estimated over part of
the Reuters-21578 news corpus\cite{lewis2004}.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=.4]{figures/e_pos_feature}
  \caption{Reference positional feature vector for the character 'e'.}
  \label{e_pos_fig}
\end{figure}

We repeat this process on the sequence of clustered components by
gathering positional cluster frequency stats using the estimated
components of the space symbol cluster to demarcate word boundaries.
By taking these counts over words of length $x$ or less, we have that
each cluster can be described as a point in this ``positional''
feature space.

Before comparing cluster feature vectors with those in our symbol
corpus, we first re-weight the symbol feature vectors based on
word-length.  Since most words in the English language are relatively
short, this will help minimize the effect of wild positional
differences in longer words (since there will typically be few long
cluster sequences in short documents).

Determining which symbol each cluster belongs to involves ordering the
observed frequency vector of each cluster in the ``positional''
feature space with the feature vectors for the known characters, as
estimated on a large reference corpus. When the match is almost exact
(in our experiments we use Euclidean distance, but a weighted
cross-entropy could also be applied), we can make an assignment
directly.  For frequently occurring symbols like lowercase vowels or
common consonants, there are often enough counts that the cluster
positional feature vectors closely match the corresponding reference
vector, and so such a match is almost always correct.  However for
infrequently occurring symbols, feature vectors exhibit high variance
(due to the domain and subject of the document for instance), thus
additional steps must be taken.

We take a greedy approach in which we assign clusters to characters
one at a time, starting with the clusters appearing most frequently in
the document. For each cluster, we consider the top matching
characters candidates according to the positional feature vector.  For
each candidate, we use the partially assigned mapping so far to match
words containing the current cluster symbol against a large word
corpus from a dictionary. The fraction of total words in the input
document containing the cluster which are matched is calculated and
provided that it exceeds a particular threshold (75\% in our
experiments), then the character candidate under consdieration is
permanently assigned to the cluster, and we move on to determine the
mapping for the next cluster.

Initially, when few symbols have been matched, many dictionary words
are potential matches for when considering the next cluster. As more
clusters are assigned symbols, it may eventually become impossible for
a particular cluster to find a character mapping that achieves a
matching word ratio exceeding the desired threshold.  In such a case,
the symbol that achieves the largest such ratio is taken to be the
correct mapping.  If multiple symbols generate the same dictionary
lookup ratio, then ties are broken by taking the symbol that has a
closer positional feature distance.  This process is repeated until
each of the clusters has been assigned a symbol.


% EXPERIMENTS %
%%%%%%%%%%%%%%%
\Section{Experiments}

To examine the information present in positional statistics, we have completed
experiments using the fine-mode fax quality images from the business letter, 
and legal document ISRI OCR datasets\cite{nartker2005}.  All 159 documents from
the business letter set, and the 10 longest documents (labelled 9460-9469) were
taken and deskewed using the library included with the Leptonica image
processing suite\cite{bloomberg2006}.

Dictionary word lookup, and output symbol positional statistics were estimated
from the first piece of the Reuters-21578 news corpus 
(reut2-000.sgm)\cite{lewis2004}.  Each article's body text was extracted and the
trailing ``Reuter'' byline was removed, leaving a total of 744,522 symbols, and
17,601 unique words (including many proper nouns, digit strings, and words with
punctuation symbols).  Our output alphabet consisted of 92 different symbols
including upper and lowercase letters, the digits 0-9, punctuation, brackets, 
and several simple arithmetic operators.

%
%As Figure \ref{acc_v_doclen_fig} clearly shows, most lower case letters can be 
%correctly recognized fairly accurately, even with a small amount of data.  
%Performance on upper case letters and digits is drastically worse, owing to 
%their relative infrequency in documents.  100\% recognition accuracy was not
%acheived until almost 600,000 characters were seen.
%
%\begin{figure}[ht]
%  \centering
%  \includegraphics[scale=.5]{figures/acc_v_doclen}
%  \caption{Percentage of ASCII codes correctly recognized versus document
%  length}
%  \label{acc_v_doclen_fig}
%\end{figure}
%

To test overall performance on short, low quality documents, the images from
the business letter dataset were used.  These documents were typically only 1
page in length, and contained 2010 characters on average.

After grouping the pages belonging to each document together and clustering 
the images using the procedure described in Section \ref{clustering_sec}, 
positional features were then calculated for each cluster.  These feature 
vectors were then compared with the feature vectors found for the output 
symbols, which were weighted based on how frequently words of that length 
appeared in the corpus.  The resultant distances were used to define an 
ordering of output symbols for each cluster.

To determine the final mapping for each cluster, vp-ratio scores were
calculated for the first symbol in the order given the partially assigned
mappings up to that point.  If it was found that this score exceeded a 
threshold of 75\%, the mapping was deemed correct, otherwise vp-ratio scores
were calculated for all symbols in order, and that which acheived the maximum
was taken.  Ties amongst vp-ratio scores were broken by taking the output
symbol with a closer positional feature distance.  This final mapping was then
combined with the cluster sequences found to generate output text, which was
compared against the corresponding ground truth to determine accuracy.

Accuracy was established for symbols on a per-class basis (lower case letters, 
upper case letters, digits, and punctuation and other symbols).  Word accuracy
counts were also taken.  Table \ref{acc_tbl}, column labelled 'B' shows the 
resultant performance when accuracies are averaged across all 159 documents.

To examine the effect clustering performance had on the acheived accuracy,
tests on the business letter dataset were re-run using the ASCII codes of the
ground truth text.  Symbols were grouped together provided they had the same
code, allowing us to assess performance on a perfectly segmented and clustered 
set of data.  As in the previous test, positional feature statistics were
computed, and compared against those from the same corpus, and vp-scores were
used to determine final mappings.  Table \ref{acc_tbl}, column 'B (ASCII)'
shows the resultant improvement in accuracy.

To determine the impact that document length has on performance we also tested 
our approach on the longest documents in the legal document dataset.  Each of 
these documents was 15 pages in length, and averaged 22,959 characters.
Results are reported when clustering the input images, and when using the ASCII
codes, and are reported in the 'L', and 'L (ASCII)' columns of Table
\ref{acc_tbl} respectively.

\begin{table}[ht]
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    & B & B (ASCII) & L & L (ASCII)\\
    \hline
    \hline
    overall & 67.84 & 88.24 & 90.72 & 96.07\\
    \hline
    low letters & 73.17 & 95.71 & 97.79 & 100\\
    \hline
    upper letters & 9.21 & 50.65 & 4.02 & 65.66\\
    \hline
    digits & 6.84 & 20.52 & 7.51 & 23.64\\
    \hline
    other sym. & 24.55 & 52.36 & 31.44 & 61.72\\
    \hline
    word & 50.50 & 79.91 & 92.49 & 95.30\\
    \hline
  \end{tabular}
  \caption{Recognition Accuracy based on symbol type for image and ASCII code
           clustered data}
  \label{acc_tbl}
\end{table}

To further illustrate performance versus document length, the per class
accuracies of each document in the business letter dataset was plotted.  The
results are showin in Figure \ref{acc_v_doclen_fig}.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=.5]{figures/acc_v_doclen}
  \caption{Percentage of ASCII codes correctly recognized versus document
  length from the business letter dataset}
  \label{acc_v_doclen_fig}
\end{figure}


% CONCLUSIONS %
%%%%%%%%%%%%%%%
\Section{Conclusions}
% does well where we'd expect (those symbols which occur most frequently,
% like lowercase letters)
%
% Often documents too short to get useful positional counts for infrequent
% characters
%
% table 1 quantifies how much info is lost due to clustering (by
% comparing to ascii ground truth)
%
% poor performance on image clustering can in part be attributed to
% underestimating space width, leaving too many short length words (particulary
% in B dataset)
%  

% REFERENCES %
%%%%%%%%%%%%%%
\bibliographystyle{latex8}
\bibliography{font_free_ocr}

\end{document}
